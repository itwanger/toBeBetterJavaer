# GLM-4.7 实测：从前端到后端，AI 真的能帮我写完代码了吗？

大家好，我是二哥呀。

如果把时间拨回到 2023 年，AI 圈讨论最多的还是：谁更会聊天、谁更像人、谁能写诗。到了 2025 年，风向开始明显变了。越来越多的人开始关心：

- 它能不能帮我把代码写完？
- 能不能读懂我这个祖传屎山项目？
- 能不能真的参与到生产环境里，帮我干活？

换句话说，大模型正在从“表演型智能”，走向“工程型智能”。

智谱刚刚发布了新一代旗舰模型 **GLM-4.7**，总参数 355B，并且专门面向 Coding 场景强化了编码、长程任务规划等能力，目前已在 Hugging Face、ModelScope 开源部署！

![榜一](https://files.mdnice.com/user/3903/8153c477-d26f-4e18-a582-a7c186e32082.jpg)

就连老外都在 x 上盛赞：GLM-4.7 超越了 Claude-Sonnet-4.5 和 GPT-5。

![](https://files.mdnice.com/user/3903/a9f9aaea-1137-4298-b02b-26dc986b30f2.jpg)

这是一个了不起的成绩！我也第一时间让它参与了真正工程级的代码编写，不管是前端还是后端，表现力都超出了我对它的预期，现在是我根据自己的体验带来的一手实测。

下面是通过 GLM-4.7 完成的一个 Agent 项目，可以实现工作流的拖拉编排。

![](https://files.mdnice.com/user/3903/76d859ed-c1f5-457b-a7b2-edfaef18fbc6.png)

## 01、使用 Claude Code 接入 GLM-4.7

Claude Code 的模型调用层，本身就是可配置、可替换的。我们只需要在 .claude/settings.json 中把 ANTHROPIC_AUTH_TOKEN 替换为你的 API Key 就可以了。

ANTHROPIC_BASE_URL 的值固定为 `https://open.bigmodel.cn/api/anthropic`。

![](https://files.mdnice.com/user/3903/b2faefbb-9476-4128-8e13-299a9366ec8c.png)

保存后，重启 Claude Code，输入 `/status`，如果看到 BigModel.cn 的身影，就说明我们配置成功了。

![](https://files.mdnice.com/user/3903/0e2ae8a6-d341-4b3c-9183-00ac273aaf7a.png)

## 02、GLM-4.7 的后端工程能力

前置环境搞定后，我们直接来新增需求，很简单一句话，我们看看 GLM-4.7 是否能够通情达理，get 到我们的诉求。

> 我现在需要在大模型节点下新增一个智谱节点

![](https://files.mdnice.com/user/3903/33109199-6cac-4113-b0a7-48c71f6db686.png)

能看到，GLM-4.7 会先探索我们的代码库，了解当前大模型节点的实现结构。

这一点至关重要，就好像我们打算出远门，总要去地图上看看路线，是吧，搞清楚后再动手，免得返工。

遇到需要权限的诉求，我们直接给它。

![](https://files.mdnice.com/user/3903/b3240935-eee8-40fa-b90a-5a1132a40289.png)

搞清楚工作流的执行引擎后，GLM-4.7 开始查看数据库和前端代码，非常严谨。

![](https://files.mdnice.com/user/3903/7972d192-c146-42a4-a450-3ab5d52e121c.png)

前后端的代码+数据库搞清楚后，开始正式写代码。真正做到了“先思考、再行动”。

![](https://files.mdnice.com/user/3903/55396f3c-88da-49b9-a7ce-2bf1ca0d3404.png)

有代码需要调整的地方，也会清楚的告诉我们。

![](https://files.mdnice.com/user/3903/ba6c1ade-1c9f-4314-923f-9de0bb765345.png)

我选择全权交给 GLM-4.7 来处理，我只要最后的结果，不过在历史上下文当中，我们也可以再次确认都在哦了哪些修改，一目了然。

![](https://files.mdnice.com/user/3903/4213fa33-9eb4-4083-b5a0-b73a7a5b0854.jpg)

和其他模型有很大的不同，GLM-4.7 非常严谨，他会自动检查还有哪些地方需要修改。

![](https://files.mdnice.com/user/3903/b30454f9-1b10-436f-9773-ada8bb2f3a21.png)

所有的任务都搞定后，会给我们列一个任务完成清单。

![](https://files.mdnice.com/user/3903/d47be429-54fe-4ceb-8ea6-6ce750326d38.png)

好，我们直接来看一下效果，完全符合我的预期啊，这可不是闹着玩的 demo 项目，而是一个完全可以运行的工作流编排项目，也是当前最火的 AI 业务之一。

![](https://files.mdnice.com/user/3903/2522c463-b952-476f-9c1f-3dc3c92fa923.png)

竟然一次性搞定了！

从整个交互体验来看，GLM-4.7 更习惯站在“任务交付”的视角，而不是“回答问题”的视角。它给出的不再是零散代码片段，而是：

- 明确的模块划分
- 可运行的代码骨架
- 清楚的依赖关系和执行顺序
- 对边界情况的主动补充说明

这种感觉很像一个工程经验老道的同事，而不是一个只会补全代码的工具。

## 03、GLM-4.7 的 bug 修复能力

考验 GLM-4.7 的时候到了，因为真正能拉开差距的地方，真不是第一次写代码的时候，而是出问题之后，能不能把 bug 给收拾干净。

接口报错、鉴权失败、参数不对、依赖冲突，这些东西不可能一次写对。

这一点上，GLM-4.7 给我的体验非常明确：它不是在“猜怎么改”，而是在顺着错误把问题走完一遍。

比如我在调试过程中遇到了一个很典型的 API 鉴权问题，请求已经发出，但返回直接失败。我没有去分析日志，也没有补充额外解释，而是把完整的错误信息原样丢给 GLM-4.7。

![](https://files.mdnice.com/user/3903/5c53d34d-112d-4da0-92bd-0893ec5d5a3d.png)

它的第一反应不是急着改代码，而是先判断：

- 是 Key 配置问题，还是 Header 拼错
- 是鉴权方式不匹配，还是请求路径有误
- 当前代码和官方示例是否存在实现差异

这个顺序，就很“工程化”。在定位到问题之后，它会明确告诉你：哪里不对、为什么不对、应该怎么改。

这里顺手分享一个非常实用的小技巧。如果你发现第一次请求发出后，还有一些信息需要补充，可以在 GLM-4.7 返回结果前直接再输入一次附加信息。

![](https://files.mdnice.com/user/3903/065c226a-f9c5-4bdb-ad76-a8994b07e577.png)

还有一种场景，在修 bug 时非常常见：当前实现和官方 demo 好像对不上。

这时候最省事的方式就是——把官方代码示例直接丢给 GLM-4.7。这其实也在考验我们本身的工程能力，能不能给 AI 一个明确的指示。

这一步的价值在于：你不需要自己一点点 diff，模型会帮你把“意图”和“实现”对齐。

![](https://files.mdnice.com/user/3903/0b841cc0-d755-4056-bade-f10a2834ae5e.png)

修完不是结束，它还会自己检查一遍。这一点，是我觉得 GLM-4.7 非常加分的地方。

当你以为事情已经结束了，它往往会多做一步：
再扫一遍有没有遗漏的地方。

- 是否还有调用链没改干净
- 是否有前端或数据库字段需要同步调整
- 是否存在潜在的边界问题

![](https://files.mdnice.com/user/3903/532dfeef-3ee2-4404-90d3-6fe8d29b898e.png)

等所有地方都确认无误之后，它才会给你一个完整的修改完成清单。

好，我们来看一下修改后的效果，非常 nice。一个完整的输入 →llm 大模型 → 超拟人合成 → 输出的工作流编排就算是完成了。

![](https://files.mdnice.com/user/3903/cd5005f4-eaaf-4198-9ea0-91aa9d3236c7.png)

## 04、GLM-4.7 的前端工程能力

现在的执行状态，是等所有节点都跑完之后，前端一次性展示结果。

但从用户体验上来说，状态流转应该是一个动态过程：

- LLM 节点开始执行 → 执行中 → 执行完成
- 接着切换到 TTS 节点 → 执行中 → 执行完成
- 最后到结束节点，整体流程结束

我们把需求直接扔给 GLM-4.7：

> 现在有个问题，我发现，执行状态、节点执行结果都是等所有节点都执行完后才显示出来的，实际上这应该是一个动态的过程，llm 节点开始执行的时候执行状态就切到 llm 节点开始执行、执行中、执行结束，然后到 tts 的开始执行、执行中、执行结束，最后到结束节点，这应该是实时响应的。前后端需要联调起来。

面对这个问题，GLM-4.7 并没有一上来就写代码，而是先明确了一点：这是一个典型的「执行状态实时推送」问题，技术上可以用 SSE，也可以用 WebSocket。

![](https://files.mdnice.com/user/3903/b5479227-d9fe-446e-96db-171d552c15d6.png)

为了方便后期能够主动中断工作流的执行，GLM-4.7 选择了 WebSocket。

![](https://files.mdnice.com/user/3903/97494a97-0e94-40ef-9ce6-7beb2479dd94.png)

GLM-4.7 并没有只盯着前端，而是从后端执行引擎开始改起：

- 后端增加 WebSocket 依赖和基础配置
- 新增 WebSocket Handler，用于推送节点执行状态
- 在执行引擎中，把“节点生命周期”拆成可感知的事件
- 在节点开始、执行中、结束等关键节点，主动向前端推送状态

后端改完之后，它才开始动前端。前端这边的改动也非常清晰：

- 建立 WebSocket 连接
- 监听后端推送的执行事件
- 根据节点 ID 实时更新节点状态
- 让画布上的节点“活”起来，而不是等结果一次性刷新

![](https://files.mdnice.com/user/3903/b68ce1a3-7b00-40b8-aa0e-0d6e92f0d78a.jpg)

有一个细节，我觉得特别加分。在节点执行完成后，GLM-4.7 顺手把节点执行结果里的 JSON 文本做了格式化展示，而不是直接把一坨字符串甩给用户。

![](https://files.mdnice.com/user/3903/e494e38a-536e-4308-bded-a88ca17ef1bb.png)

这个动作看起来很小，但非常贴心。

我录了一个完整的视频，大家可以看看，非常完美。从输入 → LLM 执行 → TTS 合成 → 输出完成，
每一步都是实时可见的。

【视频】

这一刻我心里其实挺笃定的：GLM-4.7 已经不只是“会写前端代码”，而是理解前端在整个系统中的位置和责任。

除了硬核的实战代码能力之外，这次的 GLM-4.7 还进一步提升了前端审美。

我们直接来让它来帮我们把用户界面改造为赛博朋克风。

![](https://files.mdnice.com/user/3903/8ed438ce-2ad4-4980-9768-b3ffb425ecda.png)

当然了，这个过程中也出现了一些其他的小问题，但好在经过我和 GLM-4.7 的通力合作，算是都解决了。来看一下最后呈现的效果吧。

![](https://files.mdnice.com/user/3903/eb753b08-4459-4190-916d-8fbdc2eeb0f8.jpg)

是不是很酷？

## 05、GLM-4.7 与 AMA、沪上阿姨

我在刷 Reddit 的时候，还看到了一些有趣的信息。智谱官方在上面举办了一场海外 AMA，由 GLM-4.7 背后的研发团队来回答一些用户关心的问题。包括模型定位、训练取舍到工程化落地等各个细节。

![](https://files.mdnice.com/user/3903/016e8853-2552-407d-947b-6952372f5b9f.png)

其中释放了一个非常关键的信号：**上市并不意味着模型节奏放缓，反而是长期投入的开始**。

GLM 团队始终把自己放在通往 AGI 的长期进程中，而不是围绕某一代模型“打完就走”。从这个角度看，GLM-4.7 更像是一次阶段性成果，而不是终点。

另外，偷偷告诉大家一个好消息。

订阅智谱 Coding Plan 的用户，在 Claude Code 等编程工具中，输入口令「**阿姨助我！**」，即刻领取一张沪上阿姨新品「QQ 美莓奶茶」兑换券。

【截图】

## 06、ending

如果只让我用一句话来总结真实体感，那就是：

**GLM-4.7，已经坐实了 Claude 的最佳平替，绝不是嘴上说说那种。** 从 GLM-4.5 出来能承接一部分编程小项目，再到 GLM-4.6 更强的编程，智谱今年在 Coding 上的发力是完全没有想到的力度。

不是在榜单里跑分，也不是写几个 demo case，而是放进真实工程、真实 Agent、真实联调环境里之后，你会明显感觉到一件事——它确实是一个称心&顺手的「工程生产力工具」了。

- 它能读懂真实项目结构，而不是只看单文件；
- 它习惯从“任务交付”而不是“回答问题”的角度出发；
- 它会主动检查遗漏、修 bug、补联调，而不是写完就走；
- 它在前后端协同、状态流转、工程约束这些地方，明显是有经验积累的。

我自己是订阅了智谱的 Coding Plan 年包计划，平常开发基本上不用担心 token 的用量问题，性价比还是非常夸张的。

![](https://files.mdnice.com/user/3903/b2584a71-d8b5-4c66-8b66-e4e74ac1e9e3.png)

- 更稳定的 Coding 专用模型调度策略
- 对长上下文、复杂任务拆解的明显优化
- 在 Claude Code、IDE 类工具里的兼容性持续增强
- 对 Agent 场景、工具调用、任务连续性的支持越来越完整

从一个开发者的角度说，这种投入方向是非常值得的。

如果你现在正好在做 Agent、做工作流、做复杂业务系统，或者你只是单纯想提升自己的工程效率、在 AI 时代把生产力再往上抬一档。

那 GLM-4.7 **绝对是一个完全站得住脚、而且很难被忽视的选择。**
